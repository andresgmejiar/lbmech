---
title: README
output: 
md_document:
    variant: markdown_github
github_document:
    toc: true
    pandoc_args: --webtex https://latex.codecogs.com/svg.latex?
pdf_document:
    keep_tex: true

---

<!-- README.md is generated from README.Rmd. Please edit that file -->

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  fig.path = "man/figures/README-",
  out.width = "100%",
  warning = FALSE,
  progress = FALSE
)
```

# lbmech

<!-- badges: start -->
<!-- badges: end -->

`lbmech` is a geospatial package for `R` to calculate time- and energy-based 
costs-of-travel for humans and animals moving across the landscape. While 
providing a similar functionality to the package `gdistance`, `lbmech` stores
data and performs all linear algebra using the `data.table` package allowing
for in-place modification of objects greatly increasing processing speed.
`lbmech` also modularizes important aspects of the cost-distance workflow
allowing for computationally-intensive and otherwise prohibitively-large
operations to take place. Moreover, unlike similar tools such as `package`
for `R`, `lbmech` allows for the estimation of various types of energetic losses
(due to kinematic locomotion, work against gravity, basal metabolic processes)
instead of simply the total energetic or metabolic expenditure. 

The workflow demonstrated in this README provides a detailed guide employing
the examples included in the individual function documentation and 
generating all data from scratch. For applied/real-world examples, please
see the forthcoming vignettes. For most purposes, only five or six function calls
to the package are necessary. A 'quick start' guide summarizing the README
example in as few function calls as possible is provided at the end of this file.


## Installation

You can install the development version from [GitHub](https://github.com/) with:

``` r
# install.packages("devtools")
devtools::install_github("andresgmejiar/lbmech")
```
## Example Workflow

```{r import}
library(lbmech)
# Set random seed for reproducibility's sake
set.seed(5574741)
```


### Part 1: Topographic data sources

The first step in a typical `lbmech` workflow is defining the digital elevation
model (DEM) to define as the topographic data source. 
This may be provided in one of two ways:

1. A RasterLayer object representing the digital elevation model for the
region-of-interest
    * In order to ensure that there is a stable file path to the source
    DEM throughout the workflow, the raster must have been 'read in'
    using the `raster` function without having been further modified. If 
    additional modifications are necessary, use the `writeRaster` function
    to save it to the disk first before re-reading it in using `raster`. 
    For example:
    
```{r dem}
# Generate a DEM
n <- 5
dem <- expand.grid(list(x = 1:(n * 100),
                        y = 1:(n * 100))) / 100
dem <- as.data.table(dem)
dem[, z := 250 * exp(-(x - n/2)^2) + 
      250 * exp(-(y - n/2)^2)]
dem <- rasterFromXYZ(dem)
extent(dem) <- c(10000, 20000, 30000, 40000)
crs(dem) <- "+proj=lcc +lat_1=48 +lat_2=33 +lon_0=-100 +datum=WGS84"

# Export it so it doesn't just exist on the memory
dir <- tempdir()
writeRaster(dem, paste0(dir,"/DEM.tif"),format="GTiff",overwrite=TRUE)


# Import raster
dem <- raster(paste0(dir,"/DEM.tif"))

plot(dem, main = "Digital Elevation Model")
```
    
2. A SpatialPolygonsDataFrame object whose individual polygons represent
sectors with unique DEM sources stored as a file path or URL in the data frame
object. 
    * As of the most recent version, `lbmech` supports file types 
    readible by `rgdal` and `raster`, as well as such files compressed in `gz` 
    and `zip` files--although the latter is likely to fail in Unix systems. 

Even if you have already downloaded or imported a raster to use as a
topographic data source as in case one above, most of the functions will
expect a SpatialPolygonsDataFrame object in the form of case two. You can
make this using the `makeGrid` function:

```{r makeGrid}
grid <- makeGrid(dem = dem, nx = n, ny = n)

plot(dem, main = "Sectors to divide DEM")
plot(grid,add=TRUE)
```

`lbmech` is specifically designed to deal with **large** regions that would 
be prohibitive to analyze if the data is stored exclusively within the memory.
To deal with this issue, `lbmech` will crop any input raster into an `nx` by `ny`
grid and save the sector in its own `gz` file for case 1. In both cases---to 
save memory and computational time---sectors are only cropped or downloaded on 
an as-needed basis. You can use the `whichTiles` and `getMap` functions
to identify which tile(s) might be needed, and download or crop any such tiles
that haven't been prepared:

```{r getMap}
# Generate five random points that fall within the grid
points <- data.table(x = runif(5, extent(dem)[1], extent(dem)[2]),
                     y = runif(5, extent(dem)[3], extent(dem)[4]))
               
                           
# Run whichTiles and getMap to prepare appropriate sector files
tile_list <- whichTiles(region = points, polys = grid) 
print(tile_list)

getMap(tiles = tile_list, polys = grid, dir = dir)
print(list.files(dir,recursive=TRUE, pattern = ".gz$"))

```
By far the most computationally intensive part of the workflow is the 
first transformation of the topographic data. To calculate the distances,
we will need to convert the data from a a matrix of locations (a raster) 
with an associated attribute (elevation) to a list of possible movements 
between locations (all raster cells and their neighbors) and associated
attribute (difference in elevation). These are stored as `gz` files in a
folder named `'Tensors'` in the workspace directory and will be later
read into the memory as required to avoid having to re-calculate them every
time they are needed:

```{r makeWorld}
# Select all tiles that exist between x = (12000,16000) and y = (32000,36000)
tiles <- extent(c(12000,16000,32000,36000))
tiles <- as(tiles,"SpatialPolygons")
crs(tiles) <- crs(grid)
tiles <- whichTiles(region = tiles, polys = grid)

makeWorld(tiles = tiles, polys = grid, cut_slope = 0.5, z_fix = dem, dir = dir)
```
`makeWorld` internally calls `getMap` so you do not need to prepare the data
beforehand. A note about the parameters. The `cut_slope` is the magnitude of the 
dimensionless slope beyond which movement is not possible. 

`z_fix` merits special attention since its consistency will be required for
most of the later functions in the workflow. `lbmech` allows for sector-defining
grids pointing to DEM sources in different coordinate reference systems, spatial
resolutions, and grid origins. However, the package requires a 'master' raster
be designated that will define the projection, spatial resolution, and grid 
origin to be used in the analyses. All data will be projected to these 
'fixed' parameters. If your sector-defining grid points to a single
raster such as the output of the `makeGrid` function, you can simply use the 
original DEM as the `z_fix` parameter throughout. If the grid is custom or 
comes from a third party, it is useful to designate a raster from within the 
dataset as the master if the spatial resolution matches the desired unit of analysis.
If not, you can make an appropriate `z_fix` using the `fix_z` function:

```{r fix_z}
z_fix <- fix_z(res = res(dem)[1],      # Desired resolution
               crs = crs(dem),         # Desired projection
               dx  = origin(dem)[1],   # Desired horizontal offset
               dy  = origin(dem)[2])   # Desired vertical offset

```

### Part 2: Velocity data sources

The fundamental assumption behind the least-cost calculations is that
humans and animals tend to prefer to walk at predictable speeds at a 
given slope. The maximum speed is generally achieved at one particular
slope, and speed would tend to decrease exponentially as the distance to
this ideal slope increases. Mathematically, this is described by Tobler's
Hiking function ${d \ell}/{d t} = v_{\mathrm{max}} e^{-k | {d z} / {d \ell} - \alpha |},$
where $\ell$ is horizontal displacement, $t$ time, ${d \ell}/{d t}$ horizontal
speed, $v_{\mathrm{max}}$ the maximum walking speed, ${d z}/{d \ell}$ the 
change in elevation versus the change in horizontal distance (dimensionless
slope, or $\arctan \theta$ where $\theta$ is the slope in degrees or radians),
$\alpha$ the ideal slope of maximum walking speed, and $k$ a parameter controling
how sensitive changes in speed are to changes in slope. Canonical applications
of this function to humans set $v_{\mathrm{max}}= 1.5$ m/s, $k = 3.5$, and
$\alpha = -0.05 = \tan{(-2.83^\circ)}$.

`lbmech` provides the getVelocity  function by which these parameters can be 
estimated from locational data for different species. Data should be structured 
such that there is a column with x coordinates, a column with y coordinates, 
a column with changes in time, and a column with a trajectory id. Note that 
all values must be in meters, and the `'x'` and `'y'` coordinates in a 
projected coordinate system.
If those columns are named anything other than `'x'`, `'y'`, '`dt`',
and `'id'`, the column names need to be declared explicitly. Elevational
data is provided as the `z` parameter. This can be either a column with 
elevations---such as those recorded by a GPS unit---a RasterLayer representing
the DEM for that region, or a SpatialPolygonsDataFrame like the output of the
`makeGrid` function:

```{r getVelocity}

# Generate dummy GPS data
# 10,000 observations falling within the extent of the generted DEM
# taken at an interval of 120 seconds between observations
# and of 10 different individuals (1000 per individual)
data <- data.table(x = runif(10000,extent(dem)[1],extent(dem)[2]),
                   y = runif(10000,extent(dem)[3],extent(dem)[4]),
                   dt = 120,
                   ID = rep(1:10,each=1000))

velocity <- getVelocity(data = data, z = grid, dir = dir)
```
```{r velocityKable, include = FALSE}
# Convert the data to knitr::kable object so it prints on Github
velocityPrint <- velocity
velocity$data <- head(data)
```
So what happened? `getVelocity` called `whichTiles` and `getMap` to identify
which tiles were needed to get the elevation for the points we generated in 
`data`. It then cropped and saved each tile in a folder named `'Elevations'`
in the `dir`. Afterwards, it extracted the elevation for each `data` point,
and performed a nonlinear quantile regression to get the appropriate parameters.

The output object is a list. Since this was calculated based on random data,
the calculated parameters here are meaningless but let's have a look at the 
structure anyways:
```{r velocity, results = 'hide', echo = TRUE}
print(velocity)
```
```{r velocityPrint, echo = FALSE}
print(velocityPrint)
```
The velocity list contains seven entries: 

1. `$model`, containing an object of class `nlrq` with the output model
from the nonlinear quantile regression (nlrq) structured in the form of Tobler's 
function. You can treat this as any other statistical model object such as `lm`.

2. `$vmax`, containing the identified maximum velocity, calculated as the
`tau_max` fraction of all observed velocities.

3. `$alpha`, containing the identified angle of maximum velocity, and calculated
from the nlrq of Tobler's function

4. `$k`, containing the identified topographic sensitivity factor, and calculated
from the nlrq of Tobler's function.

5. `$tau_max`, containing the employed `tau_max`.

6. `$tau_nlrq`, containing the employed `tau_nlrq`.

7. `$data`, containing a data.table with the original data in a standardized format

### Part 3: Preparing the World
You'll have noticed that `makeWorld` simply made the `gz` files in the working
directory for each sector that's required. To import them into the memory,
use the `importWorld` function. It first runs `makeWorld` (which in turn
calls `getMap`) to make sure each necessary sector has been prepared,
and then imports ONLY the possible movements falling within a given `region`
and excluding those falling within a given `banned` area:

```{r importWorld, results='hide'}
# Import the data lying between x = (12000,16000) and y = (32000,36000)...
region <- extent(c(12000,16000,32000,36000))
region <- as(region,"SpatialPolygons")
crs(region) <- crs(grid)

world <- importWorld(region = region, polys = grid, banned = NULL,
                     cut_slope = 0.5, z_fix = dem, dir = dir)

```
Let's have a look at what the `world` data.table looks like:

```{r world}
head(world)
```
There are three columns. `$from` and `$to` contain the x and y coordinates for
the start and stop of each possible movement/transition. These are stored
as character strings, with a precision of up to two decimal points depending
on the resolution and origin. `$dz` contains the change in elevation encountered
when traveling from the `$from` cell to the `$to` cell. 

The next step is calculating the cost in terms of time, work, and energy for
every possible transition. The `calculateCosts` function takes the changes in
elevation and using the velocity information from the previous section,
models of biomechanical work expediture, and physical limitations calculates
the expected costs. There are currently three available models, run 
`?calculateCosts` for more information on each model and what parameters are
required. This is for a 60 kg human with a maximum walking speed of 1.5 m/s,
a leg length of 80 cm, a stride length of 1.6 m, a BMR of 93 J/s, and canonical
values for Tobler's hiking function:

```{r calculateCosts}
world <- calculateCosts(world = world, method = 'kuo', m = 60, v_max = 1.5,
                        BMR = 93, k = 3, alpha = -0.05, l_s = 1.6, L = 0.8)
```
Note that we could simply have done
`v_max = velocity$vmax, alpha = velocity$alpha, k = velocity$k`, but the 
current `velocity` object was generated with random data and thus the parameters 
are nonsensical. Taking a look at the `world` object now shows an additional
nine columns:

```{r costs}
head(world)
```

`$x_i` and `$y_i` give the numeric x and y coordinates of the
first part of the movement/transition. `$dl` gives the distance, `$dl_t` the
predicted speed, `$dt` the predicted amount of time spent making that movement,
`$dU_l` the work performed against gravity, `$dK_l` the kinematic work performed,
`$dW_l` the net mechanical work performed, and `$dE_l` the total energetic/metabolic
expenditure. 

### Part 4: Getting Costs, Paths, and Corridors

The final part of the workflow involves calculating the minimum cost and/or least-cost
path between two sets of points. Generally the first step in this process is 
running the `getCosts` function, with the parameters set based on your needs:

1. If you simply desire the distance between two sets of points (cases 1 and 2), 
provide entries for `from` and `to` (or just `from` if the interest is in all 
distances between locations in that object). Output is a distance matrix. 
The computational time for this operation is comparable to generating a raster 
for the distance to all cells in the world (unless all of the locations in the 
object are close to each other). So unless the operation is to be done multiple 
times, it is highly recommended to generate the raster as below and extract 
values:

```{r getCosts-matrix, results='hide'}
# Generate five random points that fall within the region
points <- data.table(ID = 1:5,
                     x = runif(5, extent(region)[1], extent(region)[2]),
                     y = runif(5, extent(region)[3], extent(region)[4]))
                     
# Get the cost for travel between all combination of points
costMatrix <- getCosts(world = world, from = points, z_fix = dem,
                       costs ='all', direction = 'out', dir = dir)
```
The output will be a list of cost matrices, with elements named after the type
of costand direction of travel:
```{r print-matrix}
print(costMatrix)
```
2. If you wish to generate a RasterStack of costs from and/or to all nodes in the 
from object, set the `output = 'object'` and `destination = 'all'`.

3. You may also save the rasters as a series of `tif` files in the same workspace 
directory as the transition `gz` tensor files and the cropped/downloaded DEMs. 
This allows us to use `getCosts` within a loop for large numbers of origin nodes 
without running into random access memory limitations. 
Do this by setting `output = 'file'` and `destination = 'all'`.

4. You may perform (2) and (3) simultaneously by setting `output = c('file','object')`
and `destination = 'all'`.
```{r getCosts-raster, results='hide'}
# Calculate the cost rasters to travel to and from a set of points
costRasters <- getCosts(world = world, from = points, z_fix = dem,
                        destination = 'all', costs = 'all',
                        output = c("object","file"), dir = dir)

```
Let's take a look at the structure of the costRasters:

```{r raster-structure}
structure(costRasters)
```
It's a list of RasterStacks, each in a slot named after the type of cost
(time, work, or energy) and the direction of travel (out from a node, or 
in to a node). Each RasterStack has one layer for each node, with the value
at each point in the RasterLayer representing the absolute minimum cost necessary
to travel between that given node to/from that given point. 
```{r raster-plot}
plot(costRasters$time_in[['To_5']],main='Time in Seconds to Travel to Point 5')
```
Since `'file'` was also listed in the `output` parameter for `getCosts`, the
raster files were also exported to a `'CostRasters'` folder in `dir`. 


```{r raster-dir}
print(list.files(normalizePath(paste0(dir,"/CostRasters"))))
```

These rasters---be they stored as an object on the memory with `'object' %in% output`
or on the hard drive with `'file' %in% output'` are required to compute cost 
corridors. For a given series of origins and destinations (e.g. A -> B -> C)
a cost raster gives the absolute minimum expectedtotal cost that would be required 
to route the path through any given point on the landscape. The cells with the 
lowest value correspond to the least-cost path, while the value of all other
cells minus the lowest value represents the cost to perform a SINGLE detour
to any given location from the least-cost path. These rasters can be
generated with the `makeCorridor` function:

```{r corridor}
# Calculating the corridors from a list of RasterStacks,
# with path 1 -> 2 -> 4 -> 1 -> 5
pathOrder <- c(1,2,5,1,4)
corridors <- makeCorridor(rasters = dir, order = pathOrder)
plot(corridors$time - minValue(corridors$time), 
     main = 'Minimum detour in Seconds to add a given point on the
     landscape to the path 1 -> 2 -> 5 -> 1 -> 4')
```

We could also have used the parameter `rasters = costRasters` since
`makeCorridor` accepts the object output of `getCosts` as its input. Note that
the names of the nodes listed in `order` must correspond to the names of the 
nodes used in the `from` column of the `getCosts` function. Due to carried 
floats it is insufficient to simply select all values corresponding to the 
minimum cost to identify the least-cost path. Fortunately, the `getPaths`
function can generate a list of SpatialLinesDataFrame objects 
corresponding to the least-cost paths:

```{r paths}
paths <- getPaths(world = world, nodes = points, z_fix = dem, order = pathOrder)

# re-plot the corridors
plot(corridors$time - minValue(corridors$time), 
     main = 'Least-Cost-Path for route 1 -> 2 -> 5 -> 1 -> 4')
plot(paths$time, add=TRUE)

```


## Quick Start
Assuming that we have (1) generated a `dem`, (2) location `points`, (3) high-resolution
locational `data` representing the XY location of an animal, and (4) a `region` of 
interest lying within the `dem`'s extent, the minimum required workflow to
get the velocity function, cost rasters for a set of location points, a
corridor between some or all of them, and the least cost path is:

```{r quickstart, results='hide'}
# Start by making the grid:
n <- 5
grid <- makeGrid(dem = dem, nx = n, ny = n)
# Or you can employ a grid from an alternate source pointing to
# filepaths or URLs from which to download data


# Get the velocity function
velocity <- getVelocity(data = data, z = grid, dir = dir)

# Calculate the transitional costs to move between each allowable cells.
# Replace the v_max, alpha, and k parameters with known values if appropriate,
# and make sure all other parameters are representative for your species of interest
world <- calculateCosts(world = grid, m = 60, v_max = velocity$vmax,
                        k = velocity$k, alpha = velocity$alpha, BMR = 93,
                        l_s = 1.6, L = 0.8, region = region, z_fix = dem,
                        cut_slope = 0.5, dir = dir)

# Get the total costs for the least-cost path between any point on the landscape
# and all of the nodes; export as rasters but don't keep the object
getCosts(world = world, from = points, z_fix = dem, destination = 'all',
         output = 'file', dir = dir)

# Calculate the corridors for the path between nodes 1 -> 2 -> 5 -> 1 -> 4
pathOrder <- c(1,2,5,1,4)
corridors <- makeCorridor(rasters = dir, order = pathOrder)

# Get that same least-cost path
paths <- getPaths(world = world, nodes = points, z_fix = dem, order = pathOrder)
```
